<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="记录一下模型加速、部署、压缩相关">
<meta property="og:type" content="article">
<meta property="og:title" content="模型部署学习">
<meta property="og:url" content="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="XBlog">
<meta property="og:description" content="记录一下模型加速、部署、压缩相关">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/netron.jpg">
<meta property="og:image" content="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/1.png">
<meta property="og:image" content="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/2.png">
<meta property="article:published_time" content="2023-04-24T02:05:40.000Z">
<meta property="article:modified_time" content="2023-04-24T02:12:16.000Z">
<meta property="article:author" content="Carey">
<meta property="article:tag" content="job">
<meta property="article:tag" content="部署">
<meta property="article:tag" content="code">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/netron.jpg">


<link rel="canonical" href="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/","path":"2023/04/24/模型部署学习/","title":"模型部署学习"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>模型部署学习 | XBlog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">XBlog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2"><span class="nav-number">1.</span> <span class="nav-text">模型部署</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#openmmlab%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B"><span class="nav-number">1.1.</span> <span class="nav-text">openMMLab部署教程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#onnx"><span class="nav-number">1.1.1.</span> <span class="nav-text">ONNX</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#onnx-runtime"><span class="nav-number">1.1.2.</span> <span class="nav-text">ONNX Runtime</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#onnx%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.1.3.</span> <span class="nav-text">ONNX底层实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorrt"><span class="nav-number">2.</span> <span class="nav-text">TensorRT</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E4%BB%8B"><span class="nav-number">2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86"><span class="nav-number">2.2.</span> <span class="nav-text">加速原理：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%99%8D%E4%BD%8E%E6%95%B0%E6%8D%AE%E7%B2%BE%E5%BA%A6"><span class="nav-number">2.2.1.</span> <span class="nav-text">1、降低数据精度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="nav-number">2.2.2.</span> <span class="nav-text">2、模型压缩</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E6%A0%B8%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4"><span class="nav-number">2.2.3.</span> <span class="nav-text">3、内核自动调整</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%BC%A0%E9%87%8F%E6%98%BE%E5%AD%98"><span class="nav-number">2.2.4.</span> <span class="nav-text">4、动态张量显存</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E6%B5%81%E6%89%A7%E8%A1%8C"><span class="nav-number">2.2.5.</span> <span class="nav-text">5、多流执行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorrt%E9%80%82%E7%94%A8%E4%BA%8E%E9%82%A3%E4%BA%9B%E5%9C%B0%E6%96%B9"><span class="nav-number">2.3.</span> <span class="nav-text">TensorRT适用于那些地方</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%89%E4%B8%AA%E9%98%B6%E6%AE%B5"><span class="nav-number">2.3.1.</span> <span class="nav-text">三个阶段</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD"><span class="nav-number">2.4.</span> <span class="nav-text">功能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#section"><span class="nav-number">3.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9-1"><span class="nav-number">4.</span> <span class="nav-text">模型压缩</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E5%92%8C%E5%89%8D%E5%90%8E%E5%A4%84%E7%90%86"><span class="nav-number">5.</span> <span class="nav-text">模型推理和前后处理</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Carey"
      src="/images/xb9919.jpg">
  <p class="site-author-name" itemprop="name">Carey</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/xb9919.jpg">
      <meta itemprop="name" content="Carey">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="XBlog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="模型部署学习 | XBlog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          模型部署学习
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2023-04-24 10:05:40 / 修改时间：10:12:16" itemprop="dateCreated datePublished" datetime="2023-04-24T10:05:40+08:00">2023-04-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/" itemprop="url" rel="index"><span itemprop="name">模型部署</span></a>
        </span>
    </span>

  
    <span id="/2023/04/24/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%AD%A6%E4%B9%A0/" class="post-meta-item leancloud_visitors" data-flag-title="模型部署学习" title="阅读次数">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span class="leancloud-visitors-count"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>记录一下模型加速、部署、压缩相关</p>
<span id="more"></span>
<h2 id="模型部署">模型部署</h2>
<h3 id="openmmlab部署教程"><a target="_blank" rel="noopener" href="https://github.com/open-mmlab/mmdeploy/blob/master/docs/zh_cn/tutorial/01_introduction_to_model_deployment.md">openMMLab部署教程</a></h3>
<h4 id="onnx">ONNX</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">message Person &#123; </span><br><span class="line">  required string name = 1; </span><br><span class="line">  required int32 id = 2; </span><br><span class="line">  optional string email = 3; </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>这段定义表示在Person这种数据类型中，必须包含 name、id 这两个字段，选择性包含 email字段。根据这份定义文件，用户就可以选择一种编程语言，定义一个含有成员变量name、id、email的Person 类，把这个类的某个实例用 Protobuf 存储成二进制文件；反之，用户也可以用二进制文件和对应的数据定义文件，读取出一个 Person类的实例。</p>
<p>而对于 ONNX ，Protobuf 的数据定义文件在其<a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/tree/main/onnx">开源</a>，这些文件定义了神经网络中模型、节点、张量的数据类型规范；而二进制文件就是我们熟悉的“.onnx"文件，每一个 onnx 文件按照数据定义规范，存储了一个神经网络的所有相关数据。</p>
<p>我们日常用pytorch的是动态图，会比较慢，相当于走一步看一步，所以先将网络编译成静态图会快很多。</p>
<p>export 函数用的就是追踪导出方法，需要给任意一组输入，让模型跑起来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    torch.onnx.export(</span><br><span class="line">        model,</span><br><span class="line">        x,</span><br><span class="line">        <span class="string">&quot;srcnn.onnx&quot;</span>,</span><br><span class="line">        opset_version=<span class="number">11</span>,</span><br><span class="line">        input_names=[<span class="string">&#x27;input&#x27;</span>],</span><br><span class="line">        output_names=[<span class="string">&#x27;output&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>参数：1、要转的模型。 2、模型的输入。3、导出的onnx文件名。 4、opset_version 表示 ONNX 算子集的版本。 5 6、输入、输出 tensor 的名称。</p>
<p>ONNX转码原理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从 PyTorch 的模型到 ONNX 的模型，本质上是一种语言上的翻译。直觉上的想法是像编译器一样彻底解析原模型的代码，记录所有控制流。但前面也讲到，我们通常只用 ONNX 记录不考虑控制流的静态图。因此，PyTorch 提供了一种叫做追踪（trace）的模型转换方法：给定一组输入，再实际执行一遍模型，即把这组输入对应的计算图记录下来，保存为 ONNX 格式。</span><br></pre></td></tr></table></figure>
<p>检查模型格式是否正确</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">onnx.checker.check_model(model)</span><br></pre></td></tr></table></figure>
<p>可视化：Netron</p>
<figure>
<img src="netron.jpg" alt="netron" /><figcaption aria-hidden="true">netron</figcaption>
</figure>
<p>每个算子记录了算子属性、图结构、权重三类信息。</p>
<ul>
<li><p>算子属性信息即图中 attributes 里的信息，对于卷积来说，算子属性包括了卷积核大小(kernel_shape)、卷积步长(strides)等内容。这些算子属性最终会用来生成一个具体的算子。</p></li>
<li><p>图结构信息指算子节点在计算图中的名称、邻边的信息。对于图中的卷积来说，该算子节点叫做 Conv_2，输入数据叫做 11，输出数据叫做 12。根据每个算子节点的图结构信息，就能完整地复原出网络的计算图。</p></li>
<li><p>权重信息指的是网络经过训练后，算子存储的权重信息。对于卷积来说，权重信息包括卷积核的权重值和卷积后的偏差值。点击图中 conv1.weight, conv1.bias 后面的加号即可看到权重信息的具体内容。</p></li>
</ul>
<h4 id="onnx-runtime"><strong>ONNX Runtime</strong></h4>
<p>推理引擎</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line"></span><br><span class="line">ort_session = onnxruntime.InferenceSession(<span class="string">&quot;srcnn.onnx&quot;</span>)</span><br><span class="line">ort_inputs = &#123;<span class="string">&#x27;input&#x27;</span>: input_img&#125;</span><br><span class="line">ort_output = ort_session.run([<span class="string">&#x27;output&#x27;</span>], ort_inputs)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">ort_output = np.squeeze(ort_output, <span class="number">0</span>)</span><br><span class="line">ort_output = np.clip(ort_output, <span class="number">0</span>, <span class="number">255</span>)</span><br><span class="line">ort_output = np.transpose(ort_output, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>]).astype(np.uint8)</span><br><span class="line">cv2.imwrite(<span class="string">&quot;face_ort.png&quot;</span>, ort_output)</span><br></pre></td></tr></table></figure>
<p><strong>模型部署完成了！</strong></p>
<p>使用的时候只要安装onnx runtime就可以了,或者利用onnx runtime编译一个可执行的应用程序(SDK?)</p>
<h4 id="onnx底层实现"><a target="_blank" rel="noopener" href="https://www.cvmart.net/community/detail/6550">ONNX底层实现</a></h4>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u012655441/article/details/120370892">案例分析</a></p>
<p>ONNX 在底层是用 Protobuf 定义的。Protobuf，全称 <strong>Protocol Buffer</strong>。使用 <strong>Protocol Buffer</strong>用户需要先写一份数据定义文件，再根据这份定义文件把数据存储进一份二进制文件。数据定义文件就是数据类，二进制文件就是数据类的实例。</p>
<p>一个节点的输入，要么是整个模型的输入，要么是之前某个节点的输出。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##完全使用ONNX的API实现output = a*x+b</span></span><br><span class="line"><span class="keyword">import</span> onnx </span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> helper</span><br><span class="line"><span class="keyword">from</span> onnx <span class="keyword">import</span> TensorProto  <span class="comment">#这里定义了protocol buffer</span></span><br><span class="line"></span><br><span class="line">a = helper.make_tensor_value_info(<span class="string">&#x27;a&#x27;</span>,TensorProto.FLOAT,[<span class="number">10</span>,<span class="number">10</span>])</span><br><span class="line">x = helper.make_tensor_value_info(<span class="string">&#x27;x&#x27;</span>, TensorProto.FLOAT, [<span class="number">10</span>, <span class="number">10</span>]) </span><br><span class="line">b = helper.make_tensor_value_info(<span class="string">&#x27;b&#x27;</span>, TensorProto.FLOAT, [<span class="number">10</span>, <span class="number">10</span>]) </span><br><span class="line">output = helper.make_tensor_value_info(<span class="string">&#x27;output&#x27;</span>, TensorProto.FLOAT, [<span class="number">10</span>, <span class="number">10</span>]) </span><br><span class="line"></span><br><span class="line">mul = helper.make_node(<span class="string">&#x27;Mul&#x27;</span>, [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;x&#x27;</span>], [<span class="string">&#x27;c&#x27;</span>]) </span><br><span class="line">add = helper.make_node(<span class="string">&#x27;Add&#x27;</span>, [<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;b&#x27;</span>], [<span class="string">&#x27;output&#x27;</span>]) </span><br><span class="line">graph = helper.make_graph([mul, add], <span class="string">&#x27;linear_func&#x27;</span>, [a, x, b], [output]) </span><br><span class="line"></span><br><span class="line">model = helper.make_model(graph)<span class="comment">#onnx.load() </span></span><br><span class="line">onnx.checker.check_model(model) </span><br><span class="line"><span class="built_in">print</span>(model) </span><br><span class="line">onnx.save(model, <span class="string">&#x27;linear_func.onnx&#x27;</span>) </span><br><span class="line"><span class="comment">## 验证</span></span><br><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">sess = onnxruntime.InferenceSession(<span class="string">&#x27;linear_func.onnx&#x27;</span>)</span><br><span class="line">a = np.random.rand(<span class="number">10</span>,<span class="number">10</span>).astype(np.float32)</span><br><span class="line">x = np.random.rand(<span class="number">10</span>,<span class="number">10</span>).astype(np.float32)</span><br><span class="line">b = np.random.rand(<span class="number">10</span>,<span class="number">10</span>).astype(np.float32)</span><br><span class="line">output = sess.run([<span class="string">&#x27;output&#x27;</span>],&#123;<span class="string">&#x27;a&#x27;</span>:a,<span class="string">&#x27;b&#x27;</span>:b,<span class="string">&#x27;x&#x27;</span>:x&#125;)</span><br><span class="line"><span class="built_in">print</span>(output[<span class="number">0</span>].shape)</span><br></pre></td></tr></table></figure>
<p><code>helper.make_tensor_value_info(name,type,shape)</code>构造出一个描述张量信息的 <code>ValueInfoProto</code> 对象</p>
<p><code>helper.make_node('Mul', ['a', 'x'], ['c'])</code>构造算子节点信息 <code>NodeProto</code>，这可以通过在 ,算子类型、输入算子名、输出算子名</p>
<p><code>helper.make_graph([mul, add], 'linear_func', [a, x, b], [output])</code> 来构造计算图 <strong>GraphProto</strong>,需要传入节点、图名、输入张量、输出张量。注意：<strong>节点要按拓扑结构输入</strong></p>
<p><code>helper.make_model(graph)</code>把计算图 <code>GraphProto</code> 封装进模型 <code>ModelProto</code></p>
<p><code>onnx.checker.check_model</code>检查onnx是否符合标准</p>
<p><code>onnx.utils.extract_model</code>子模型提取的函数，它的参数分别是原模型路径、输出模型路径、子模型的输入边（输入张量）、子模型的输出边（输出张量）</p>
<h2 id="tensorrt">TensorRT</h2>
<h3 id="简介">简介</h3>
<p>Tensor是一个有助于在NVIDIA图形处理单元（GPU）上高性能推理c++库。它旨在与TesnsorFlow、Caffe、Pytorch以及MXNet等训练框架以互补的方式进行工作，专门致力于在GPU上快速有效地进行网络推理。</p>
<p>衡量软件的关键因素：</p>
<ul>
<li>吞吐量</li>
<li>效率</li>
<li>延迟性</li>
<li>准确性</li>
<li>内存使用情况</li>
</ul>
<p>在训练了神经网络之后，<strong>TensorRT</strong>可以对网络进行压缩、优化以及运行时部署，并且没有框架的开销。<strong>TensorRT</strong>通过combines layers，kernel优化选择，以及根据指定的精度执行归一化和转换成最优的matrix math方法，改善网络的延迟、吞吐量以及效率。</p>
<h3 id="加速原理">加速原理：</h3>
<h4 id="降低数据精度">1、降低数据精度</h4>
<p>模型训练通常使用 32 位或 16 位数据。而<strong>TensorRT</strong>支持kFLOAT（float32）、kHALF（float16）、kINT8（int8）三种精度的计算。<strong>可以通过低精度进行网络推理，达到加速的目的</strong>。但是该方法，会对推理精度有一定影响。</p>
<h4 id="模型压缩">2、模型压缩</h4>
<p><strong>TensorRT</strong>对网络结构进行重构，把一些能合并的运算合并在一起，从而进行加速。合并方式主要为以下两种： 1）垂直合并</p>
<ul>
<li><p>垂直方向，合并一些网络层。例如，把Conv、BN、Relu三个层融合为一个层CBR。</p>
<p><strong>conv</strong>: <span class="math display">\[\hat x = W_{conv}x+b_{conv}\]</span></p>
<p><strong>BN</strong>: <span class="math display">\[y = (\hat x - \frac{\hat x_{mean}}{\sqrt{(\hat x_{var}}  + \varepsilon)}) * \gamma + \beta = W_{bn}\hat x+b_{bn}\]</span></p>
<p><strong>ReLU</strong>:<span class="math display">\[\hat y = max\{0,y\} = max\{0,W_{bn}(W_{conv}x+b_{conv})+b_{bn}\} = max\{0,W_{bn}W_{conv}x+W_{bn}b_{conv}+b_{bn}\}\]</span></p></li>
</ul>
<pre><code>2）水平合并</code></pre>
<ul>
<li>将输入为相同张量和执行相同操作的层融合在一起。</li>
</ul>
<figure>
<img src="1.png" alt="image1" /><figcaption aria-hidden="true">image1</figcaption>
</figure>
<figure>
<img src="2.png" alt="image1" /><figcaption aria-hidden="true">image1</figcaption>
</figure>
<h4 id="内核自动调整">3、内核自动调整</h4>
<p>根据不同的显卡构架、SM数量、内核频率等(例如1080TI和2080TI)，选择不同的优化策略以及计算方式，寻找最合适当前构架的计算方式</p>
<h4 id="动态张量显存">4、动态张量显存</h4>
<p>我们都知道，显存的开辟和释放是比较耗时的，通过调整一些策略可以减少模型中这些操作的次数，从而可以减少模型运行的时间</p>
<h4 id="多流执行">5、多流执行</h4>
<p>使用CUDA中的stream技术，最大化实现并行操作</p>
<h3 id="tensorrt适用于那些地方">TensorRT适用于那些地方</h3>
<h4 id="三个阶段">三个阶段</h4>
<p>通常，开发和部署一个深度学习模型的工作流程分为了<strong>3个阶段</strong>：</p>
<ul>
<li><p>第一个阶段是<strong>训练模型</strong>【在该阶段一般都不会使用TensorRT训练任何模型】</p>
<p>在训练阶段，通常会先确定自己需要解决的问题，网络的输入输出，以及网络的损失函数，然后再设计网络结构，接下来就是根据自己的需求去整理、扩充training data，validation data and test data。在训练模型的过程中，我们一般都会通过监视模型的整个训练流程来确定自己是否需要修改网络的损失函数、训练的超参数以及数据集的增强。最后我们会使用validation data对trained model进行性能评估。需要注意的是，在该阶段一般都不会使用TensorRT训练任何模型。</p></li>
<li><p>第二个阶段是开发一个<strong>部署解决方案</strong></p>
<p>在这个阶段，我们将会通过使用trained model来创建和验证部署解决方案，该阶段分为以下几个步骤：</p>
<p>1、首先需要考虑清楚神经网络在系统中是如何进行工作的，根据需求中的优先事项设计出适应的解决方案。另一方面，由于不同系统之间存在多样性的原因，我们在设计和实现部署结构时需要考虑很多方面的因素。【例如，是单个网络还是多个网络，需要哪些后处理步骤等等之内的因素】</p>
<p>2、当设计好解决方案后，我们便可以使用<strong>TensorRT</strong>从保存的网络模型中构建一个<strong>inference engine</strong>。由于在training model期间可以选择不同的framework，因此，我们需要根据不同框架的格式，使用相应的解析器将保存的模型<strong>转换为TensorRT的格式</strong>。</p>
<p>3、model解析成功后，我们需要考虑<strong>优化选项</strong>——batch size、工作空间大小、混合精度和动态形状的边界，这些选项被选择并指定为TensorRT构建步骤的一部分，在此步骤中，您将基于网络构建一个<strong>优化的推理引擎</strong>。</p>
<p>4、使用TensorRT创建inference engine后，我们需要验证它是否可以<strong>复现原始模型的性能评估结果</strong>。如果我们选择了FP32或FP16，那么它与原始结果非常接近。如果选择了INT8，那么它与原始的结果会有一些差距。</p>
<p>5、一序列化格式保存inference engine-----called plan file</p></li>
<li><p>第三个阶段是使用开发的解决方案进行<strong>部署</strong>【即使用阶段2中的解决方案来进行部署】</p>
<p>该<strong>TensorRT</strong>库将被链接到<strong>部署应用程序</strong>，当应用程序需要一个推理结果时将会调用该库。为了初始化inference engine，应用程序首先会从plan file中反序列化为一个inference engine。另一方面，TensorRT通常是<strong>异步使用</strong>的，因此，当输入数据到达时，程序调用带有输入缓冲区和TensorRT放置结果的缓冲区的enqueue函数。</p>
<p><strong>总的来说就是正常训模型-----&gt;用TensorTR将网络转成对应的inference engine（类似om?）-----&gt;设计优化选项(batchsize,精度)-----&gt;验证性能、精度，不行就调优</strong></p></li>
</ul>
<h3 id="功能">功能</h3>
<ul>
<li><p>网络可以直接从<strong>Caffe</strong>导入，也可以通过<strong>UFF</strong>(貌似是转换tensorflow的)或<strong>ONNX</strong>格式从其他框架导入，也可以通过实例化各个图层并直接设置参数和weight以编程的方式创建。</p></li>
<li><p>可以通过<strong>TensorRT</strong>使用<strong>Plugin interface</strong>运行自定义图层。</p></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356072366"><em>参考</em></a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/371239130"><em>TensorRT详细入门指北，如果你还不了解TensorRT，过来看看吧！</em></a></p>
<h2 id="section"></h2>
<h2 id="模型压缩-1">模型压缩</h2>
<p>模型压缩是<strong>对已经训练好的深度模型进行精简，进而得到一个轻量且准确率相当的网络</strong>，压缩后的网络具有更小的结构和更少的参数，可以有效降低计算和存储开销，便于部署在受限的硬件环境中。 训练的时候因为要保证前后向传播，每次梯度的更新是很微小的，这个时候需要相对较高的精度，一般来说需要float型，如FP32，32位的浮点型来处理数据，但是在推理（Inference）的时候，对精度的要求没有那么高，很多研究表明可以用低精度，如半长（16）的float型，即FP16，也可以用8位的整型（INT8）来做推理（Inference）。所以，一般来说，在模型部署时会对模型进行压缩。模型压缩方法有：蒸馏，剪枝，量化等。</p>
<h2 id="模型推理和前后处理"><strong>模型推理和前后处理</strong></h2>
<p><strong>前处理</strong>：因为模型推理的输入是Tensor（多维矩阵）数据，但是正常AI应用的输入都是图片，视频，文字等数据，<strong>所以前处理就是要将业务的输入数据（图像，视频，文字等）预先处理成模型推理可以接收的数据---Tensor（多维矩阵）</strong>。以图像处理为例，前处理动作就包括但不限于：图像格式转换，颜色空间变换，图像变换（resize，warpaffine（仿射变换）），图像滤波等操作。</p>
<p><strong>模型推理</strong>：模型推理应该是模型部署pipline中最核心的部分。就是需要在实际应用环境中（具体部署设备）将实际输入的数据（转换成Tensor数据后）在训练好的模型中<strong>跑通</strong>，并且性能和精度等商业指标上达到预期效果。这个过程包括了对部署设备的适配（CPU/GPU/DSP/NPU），要想将模型跑在任何一种设备上，都需要提前针对设备进行适配，并且还要保证性能和精度等指标。这是个非常复杂的过程，后续希望出文章详细介绍</p>
<p>市面上有非常多的开源深度学习推理框架都是在解决模型推理相关的问题。例如：国内各大厂推出的开源的推理框架：OpenPPL、NCNN、TNN、MNN、PaddleLite、Tengine等等，还有NVIDIA推出的针对GPU卡的TensorRT、intel针对intel芯片的OpenVINO等。</p>
<p><strong>后处理</strong>：就是将模型推理后的Tensor数据转换成业务可以识别的特征数据（不同的业务会呈现不同的最终效果数据）。</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/job/" rel="tag"># job</a>
              <a href="/tags/%E9%83%A8%E7%BD%B2/" rel="tag"># 部署</a>
              <a href="/tags/code/" rel="tag"># code</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/04/24/python%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" rel="prev" title="python常见问题">
                  <i class="fa fa-chevron-left"></i> python常见问题
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Carey</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"jQYmuHNxiTiELvFQ61jSnJiJ-gzGzoHsz","app_key":"KKRkMLi4jYYuy2ApenFt3gI8","server_url":null,"security":true}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"per_page":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
